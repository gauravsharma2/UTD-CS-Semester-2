{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb504459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import builtins\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb1535d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and process the text\n",
    "def process_text(text):\n",
    "    tokens = wordpunct_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords.words(\"english\") and token not in punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ee1bb",
   "metadata": {},
   "source": [
    "# Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7107d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "que1a = [\"cranfield0727\", \"cranfield0877\", \"cranfield0895\"]\n",
    "que1b = [\"semivertex\", \"replaced\", \"entrance\"]\n",
    "que1c = [\"desired\", \"eliminated\", \"comprised\"]\n",
    "\n",
    "# que1a = ['cranfield0633', 'cranfield1346', 'cranfield0092']\n",
    "# que1b = ['torsion', 'meridional' , 'reducing']\n",
    "# que1c = ['tolerance', 'convergent', 'constraint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caab4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1A\n",
      "--------------------------------------------------\n",
      "cranfield0727\n",
      "doclen: The total number of lemmas in the document: 120\n",
      "max_tf: The frequency of the most frequent lemma in the document: 6\n",
      "unique_terms: The number of unique lemmas in the document: 78\n",
      "==================================================\n",
      "cranfield0877\n",
      "doclen: The total number of lemmas in the document: 38\n",
      "max_tf: The frequency of the most frequent lemma in the document: 4\n",
      "unique_terms: The number of unique lemmas in the document: 25\n",
      "==================================================\n",
      "cranfield0895\n",
      "doclen: The total number of lemmas in the document: 60\n",
      "max_tf: The frequency of the most frequent lemma in the document: 4\n",
      "unique_terms: The number of unique lemmas in the document: 41\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and lemmatize the text\n",
    "def tokenize_and_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    title = soup.find('title').get_text() if soup.find('title') else ''\n",
    "    text = soup.find('text').get_text() if soup.find('text') else ''\n",
    "\n",
    "    # Tokenize title and text\n",
    "    tokens = process_text(title) + process_text(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Function to process each document and update the index\n",
    "def process_document(filename, doc_id, index):\n",
    "    with builtins.open(f'Cranfield/{filename}', 'r') as file:\n",
    "        content = file.read()\n",
    "        tokens = tokenize_and_lemmatize(content)\n",
    "\n",
    "        # Update document-level information\n",
    "        max_tf = max(tokens.count(lemma) for lemma in set(tokens))\n",
    "        doclen = len(tokens)\n",
    "        unique_terms = len(set(tokens))\n",
    "        \n",
    "        if filename in que1a:\n",
    "            print(filename)\n",
    "            print(f\"doclen: The total number of lemmas in the document: {doclen}\")\n",
    "            print(f\"max_tf: The frequency of the most frequent lemma in the document: {max_tf}\")\n",
    "            print(f\"unique_terms: The number of unique lemmas in the document: {unique_terms}\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "        # Update lemma-level information\n",
    "        for lemma in set(tokens):\n",
    "            index[lemma]['df'] += 1\n",
    "            if doc_id not in index[lemma]['documents']:\n",
    "                index[lemma]['documents'].append(doc_id)\n",
    "            index[lemma]['posting_list'].append({\n",
    "                'doc_id': doc_id,\n",
    "                'tf': tokens.count(lemma),\n",
    "                'max_tf': max_tf,\n",
    "                'doclen': doclen,\n",
    "                'unique_terms': unique_terms\n",
    "            })\n",
    "            \n",
    "# Initialize the index\n",
    "index = defaultdict(lambda: {'df': 0, 'documents': [], 'posting_list': []})\n",
    "print(\"Question 1A\")\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "# Path to the Cranfield collection\n",
    "cranfield_path = 'Cranfield/'\n",
    "\n",
    "# Iterate over each document in the Cranfield collection\n",
    "for filename in os.listdir(cranfield_path):\n",
    "    if filename.startswith('cranfield'):\n",
    "        doc_id = int(re.search(r'\\d+', filename).group())  # Extract document ID from the filename\n",
    "        process_document(filename, doc_id, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78926f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que1B\n",
      "--------------------------------------------------\n",
      "semivertex\n",
      "{'df': 8, 'documents': [63, 522, 814, 936, 937, 947, 1001, 1231], 'posting_list': [{'doc_id': 63, 'tf': 1, 'max_tf': 4, 'doclen': 94, 'unique_terms': 65}, {'doc_id': 522, 'tf': 1, 'max_tf': 6, 'doclen': 229, 'unique_terms': 125}, {'doc_id': 814, 'tf': 1, 'max_tf': 5, 'doclen': 123, 'unique_terms': 82}, {'doc_id': 936, 'tf': 3, 'max_tf': 8, 'doclen': 90, 'unique_terms': 62}, {'doc_id': 937, 'tf': 1, 'max_tf': 4, 'doclen': 68, 'unique_terms': 51}, {'doc_id': 947, 'tf': 2, 'max_tf': 11, 'doclen': 198, 'unique_terms': 94}, {'doc_id': 1001, 'tf': 2, 'max_tf': 4, 'doclen': 79, 'unique_terms': 53}, {'doc_id': 1231, 'tf': 1, 'max_tf': 5, 'doclen': 102, 'unique_terms': 69}]}\n",
      "==================================================\n",
      "replaced\n",
      "{'df': 17, 'documents': [25, 101, 115, 180, 227, 332, 435, 493, 601, 624, 683, 765, 808, 842, 962, 1375, 1382], 'posting_list': [{'doc_id': 25, 'tf': 1, 'max_tf': 8, 'doclen': 223, 'unique_terms': 145}, {'doc_id': 101, 'tf': 1, 'max_tf': 8, 'doclen': 200, 'unique_terms': 126}, {'doc_id': 115, 'tf': 1, 'max_tf': 4, 'doclen': 105, 'unique_terms': 77}, {'doc_id': 180, 'tf': 1, 'max_tf': 3, 'doclen': 36, 'unique_terms': 22}, {'doc_id': 227, 'tf': 1, 'max_tf': 6, 'doclen': 89, 'unique_terms': 61}, {'doc_id': 332, 'tf': 1, 'max_tf': 9, 'doclen': 113, 'unique_terms': 68}, {'doc_id': 435, 'tf': 1, 'max_tf': 6, 'doclen': 116, 'unique_terms': 76}, {'doc_id': 493, 'tf': 1, 'max_tf': 6, 'doclen': 173, 'unique_terms': 87}, {'doc_id': 601, 'tf': 1, 'max_tf': 4, 'doclen': 130, 'unique_terms': 92}, {'doc_id': 624, 'tf': 1, 'max_tf': 6, 'doclen': 146, 'unique_terms': 99}, {'doc_id': 683, 'tf': 1, 'max_tf': 8, 'doclen': 158, 'unique_terms': 89}, {'doc_id': 765, 'tf': 1, 'max_tf': 3, 'doclen': 83, 'unique_terms': 59}, {'doc_id': 808, 'tf': 1, 'max_tf': 5, 'doclen': 109, 'unique_terms': 77}, {'doc_id': 842, 'tf': 1, 'max_tf': 3, 'doclen': 38, 'unique_terms': 29}, {'doc_id': 962, 'tf': 1, 'max_tf': 8, 'doclen': 235, 'unique_terms': 157}, {'doc_id': 1375, 'tf': 1, 'max_tf': 9, 'doclen': 181, 'unique_terms': 113}, {'doc_id': 1382, 'tf': 2, 'max_tf': 10, 'doclen': 163, 'unique_terms': 92}]}\n",
      "==================================================\n",
      "entrance\n",
      "{'df': 4, 'documents': [340, 426, 964, 1139], 'posting_list': [{'doc_id': 340, 'tf': 2, 'max_tf': 2, 'doclen': 40, 'unique_terms': 33}, {'doc_id': 426, 'tf': 3, 'max_tf': 6, 'doclen': 89, 'unique_terms': 61}, {'doc_id': 964, 'tf': 2, 'max_tf': 4, 'doclen': 89, 'unique_terms': 65}, {'doc_id': 1139, 'tf': 2, 'max_tf': 7, 'doclen': 58, 'unique_terms': 36}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que1B\")\n",
    "print('-'*50)\n",
    "for lemma in que1b:\n",
    "    print(lemma)\n",
    "    print(index[lemma])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0412f397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que1C\n",
      "--------------------------------------------------\n",
      "desired\n",
      "{'df': 8, 'documents': [17, 244, 441, 659, 679, 738, 1224, 1377], 'posting_list': [{'doc_id': 17, 'tf': 1, 'max_tf': 4, 'doclen': 79, 'unique_terms': 60}, {'doc_id': 244, 'tf': 1, 'max_tf': 19, 'doclen': 261, 'unique_terms': 184}, {'doc_id': 441, 'tf': 1, 'max_tf': 7, 'doclen': 161, 'unique_terms': 108}, {'doc_id': 659, 'tf': 1, 'max_tf': 6, 'doclen': 76, 'unique_terms': 54}, {'doc_id': 679, 'tf': 1, 'max_tf': 5, 'doclen': 97, 'unique_terms': 69}, {'doc_id': 738, 'tf': 1, 'max_tf': 7, 'doclen': 84, 'unique_terms': 59}, {'doc_id': 1224, 'tf': 1, 'max_tf': 4, 'doclen': 144, 'unique_terms': 112}, {'doc_id': 1377, 'tf': 1, 'max_tf': 3, 'doclen': 60, 'unique_terms': 52}]}\n",
      "==================================================\n",
      "eliminated\n",
      "{'df': 9, 'documents': [342, 364, 595, 643, 849, 873, 933, 1196, 1345], 'posting_list': [{'doc_id': 342, 'tf': 1, 'max_tf': 7, 'doclen': 129, 'unique_terms': 97}, {'doc_id': 364, 'tf': 1, 'max_tf': 8, 'doclen': 214, 'unique_terms': 130}, {'doc_id': 595, 'tf': 1, 'max_tf': 9, 'doclen': 124, 'unique_terms': 80}, {'doc_id': 643, 'tf': 1, 'max_tf': 5, 'doclen': 79, 'unique_terms': 57}, {'doc_id': 849, 'tf': 1, 'max_tf': 4, 'doclen': 52, 'unique_terms': 45}, {'doc_id': 873, 'tf': 1, 'max_tf': 6, 'doclen': 101, 'unique_terms': 71}, {'doc_id': 933, 'tf': 1, 'max_tf': 6, 'doclen': 159, 'unique_terms': 123}, {'doc_id': 1196, 'tf': 1, 'max_tf': 5, 'doclen': 60, 'unique_terms': 45}, {'doc_id': 1345, 'tf': 1, 'max_tf': 3, 'doclen': 70, 'unique_terms': 52}]}\n",
      "==================================================\n",
      "comprised\n",
      "{'df': 5, 'documents': [78, 185, 189, 583, 761], 'posting_list': [{'doc_id': 78, 'tf': 1, 'max_tf': 5, 'doclen': 120, 'unique_terms': 90}, {'doc_id': 185, 'tf': 1, 'max_tf': 12, 'doclen': 185, 'unique_terms': 112}, {'doc_id': 189, 'tf': 1, 'max_tf': 12, 'doclen': 236, 'unique_terms': 95}, {'doc_id': 583, 'tf': 1, 'max_tf': 4, 'doclen': 80, 'unique_terms': 62}, {'doc_id': 761, 'tf': 1, 'max_tf': 4, 'doclen': 49, 'unique_terms': 35}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que1C\")\n",
    "print('-'*50)\n",
    "for lemma in que1c:\n",
    "    print(lemma)\n",
    "    print(index[lemma])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0759a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1B\n",
      "--------------------------------------------------\n",
      "lemma: semivertex\n",
      "df: The number of documents that the lemma occurs in: 8\n",
      "The first document ID in the posting list of the lemma: 63\n",
      "The first document's tf in the posting list of the lemma: 1\n",
      "The last document ID in the posting list of the lemma: 1231\n",
      "The last document's tf in the posting list of the lemma: 1\n",
      "==================================================\n",
      "lemma: replaced\n",
      "df: The number of documents that the lemma occurs in: 17\n",
      "The first document ID in the posting list of the lemma: 25\n",
      "The first document's tf in the posting list of the lemma: 1\n",
      "The last document ID in the posting list of the lemma: 1382\n",
      "The last document's tf in the posting list of the lemma: 2\n",
      "==================================================\n",
      "lemma: entrance\n",
      "df: The number of documents that the lemma occurs in: 4\n",
      "The first document ID in the posting list of the lemma: 340\n",
      "The first document's tf in the posting list of the lemma: 2\n",
      "The last document ID in the posting list of the lemma: 1139\n",
      "The last document's tf in the posting list of the lemma: 2\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 1B\")\n",
    "print('-'*50)\n",
    "\n",
    "for term in que1b:\n",
    "    print(f\"lemma: {term}\")\n",
    "    print(f\"df: The number of documents that the lemma occurs in: {index[term]['df']}\")\n",
    "    print(f\"The first document ID in the posting list of the lemma: {index[term]['documents'][0]}\")\n",
    "    print(f\"The first document's tf in the posting list of the lemma: {index[term]['posting_list'][0]['tf']}\")\n",
    "    print(f\"The last document ID in the posting list of the lemma: {index[term]['documents'][-1]}\")\n",
    "    print(f\"The last document's tf in the posting list of the lemma: {index[term]['posting_list'][-1]['tf']}\")\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b157056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert posting list to compressed format\n",
    "def compress_posting_list(posting_list):\n",
    "    compressed_list = [posting_list[0]]  # First element remains unchanged\n",
    "    for i in range(1, len(posting_list)):\n",
    "        gap = posting_list[i] - posting_list[i-1]\n",
    "        compressed_list.append(gap)\n",
    "    return compressed_list\n",
    "\n",
    "# Convert all posting lists in the index to compressed format\n",
    "for lemma in index:\n",
    "    index[lemma]['documents'] = compress_posting_list(index[lemma]['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e997d6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que1C\n",
      "--------------------------------------------------\n",
      "desired\n",
      "{'df': 8, 'documents': [17, 227, 197, 218, 20, 59, 486, 153], 'posting_list': [{'doc_id': 17, 'tf': 1, 'max_tf': 4, 'doclen': 79, 'unique_terms': 60}, {'doc_id': 244, 'tf': 1, 'max_tf': 19, 'doclen': 261, 'unique_terms': 184}, {'doc_id': 441, 'tf': 1, 'max_tf': 7, 'doclen': 161, 'unique_terms': 108}, {'doc_id': 659, 'tf': 1, 'max_tf': 6, 'doclen': 76, 'unique_terms': 54}, {'doc_id': 679, 'tf': 1, 'max_tf': 5, 'doclen': 97, 'unique_terms': 69}, {'doc_id': 738, 'tf': 1, 'max_tf': 7, 'doclen': 84, 'unique_terms': 59}, {'doc_id': 1224, 'tf': 1, 'max_tf': 4, 'doclen': 144, 'unique_terms': 112}, {'doc_id': 1377, 'tf': 1, 'max_tf': 3, 'doclen': 60, 'unique_terms': 52}]}\n",
      "==================================================\n",
      "eliminated\n",
      "{'df': 9, 'documents': [342, 22, 231, 48, 206, 24, 60, 263, 149], 'posting_list': [{'doc_id': 342, 'tf': 1, 'max_tf': 7, 'doclen': 129, 'unique_terms': 97}, {'doc_id': 364, 'tf': 1, 'max_tf': 8, 'doclen': 214, 'unique_terms': 130}, {'doc_id': 595, 'tf': 1, 'max_tf': 9, 'doclen': 124, 'unique_terms': 80}, {'doc_id': 643, 'tf': 1, 'max_tf': 5, 'doclen': 79, 'unique_terms': 57}, {'doc_id': 849, 'tf': 1, 'max_tf': 4, 'doclen': 52, 'unique_terms': 45}, {'doc_id': 873, 'tf': 1, 'max_tf': 6, 'doclen': 101, 'unique_terms': 71}, {'doc_id': 933, 'tf': 1, 'max_tf': 6, 'doclen': 159, 'unique_terms': 123}, {'doc_id': 1196, 'tf': 1, 'max_tf': 5, 'doclen': 60, 'unique_terms': 45}, {'doc_id': 1345, 'tf': 1, 'max_tf': 3, 'doclen': 70, 'unique_terms': 52}]}\n",
      "==================================================\n",
      "comprised\n",
      "{'df': 5, 'documents': [78, 107, 4, 394, 178], 'posting_list': [{'doc_id': 78, 'tf': 1, 'max_tf': 5, 'doclen': 120, 'unique_terms': 90}, {'doc_id': 185, 'tf': 1, 'max_tf': 12, 'doclen': 185, 'unique_terms': 112}, {'doc_id': 189, 'tf': 1, 'max_tf': 12, 'doclen': 236, 'unique_terms': 95}, {'doc_id': 583, 'tf': 1, 'max_tf': 4, 'doclen': 80, 'unique_terms': 62}, {'doc_id': 761, 'tf': 1, 'max_tf': 4, 'doclen': 49, 'unique_terms': 35}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que1C\")\n",
    "print('-'*50)\n",
    "for lemma in que1c:\n",
    "    print(lemma)\n",
    "    print(index[lemma])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "935e5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Gamma encoding for a given integer\n",
    "def gamma_encode(number):\n",
    "    binary_rep = bin(number)[2:]\n",
    "    offset = binary_rep[1:]\n",
    "    offset_length = len(offset)\n",
    "    unary_code = '1' * offset_length + '0'\n",
    "    gamma_code = unary_code + offset\n",
    "    return gamma_code\n",
    "\n",
    "# Function to perform Gamma encoding on a gap compressed posting list\n",
    "def gamma_encode_posting_list(posting_list):\n",
    "    gamma_encoded_list = []\n",
    "    for gap in posting_list:\n",
    "        # Gamma encode the gap value and append to the encoded list\n",
    "        gamma_encoded_gap = gamma_encode(gap)\n",
    "        gamma_encoded_list.append(gamma_encoded_gap)\n",
    "    return gamma_encoded_list\n",
    "\n",
    "# Apply gamma encoding to the gap compressed posting lists in the inverted index\n",
    "for lemma in index:\n",
    "    index[lemma]['documents'] = gamma_encode_posting_list(index[lemma]['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b3b0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que1C\n",
      "--------------------------------------------------\n",
      "desired\n",
      "{'df': 8, 'documents': ['111100001', '111111101100011', '111111101000101', '111111101011010', '111100100', '11111011011', '11111111011100110', '111111100011001'], 'posting_list': [{'doc_id': 17, 'tf': 1, 'max_tf': 4, 'doclen': 79, 'unique_terms': 60}, {'doc_id': 244, 'tf': 1, 'max_tf': 19, 'doclen': 261, 'unique_terms': 184}, {'doc_id': 441, 'tf': 1, 'max_tf': 7, 'doclen': 161, 'unique_terms': 108}, {'doc_id': 659, 'tf': 1, 'max_tf': 6, 'doclen': 76, 'unique_terms': 54}, {'doc_id': 679, 'tf': 1, 'max_tf': 5, 'doclen': 97, 'unique_terms': 69}, {'doc_id': 738, 'tf': 1, 'max_tf': 7, 'doclen': 84, 'unique_terms': 59}, {'doc_id': 1224, 'tf': 1, 'max_tf': 4, 'doclen': 144, 'unique_terms': 112}, {'doc_id': 1377, 'tf': 1, 'max_tf': 3, 'doclen': 60, 'unique_terms': 52}]}\n",
      "==================================================\n",
      "eliminated\n",
      "{'df': 9, 'documents': ['11111111001010110', '111100110', '111111101100111', '11111010000', '111111101001110', '111101000', '11111011100', '11111111000000111', '111111100010101'], 'posting_list': [{'doc_id': 342, 'tf': 1, 'max_tf': 7, 'doclen': 129, 'unique_terms': 97}, {'doc_id': 364, 'tf': 1, 'max_tf': 8, 'doclen': 214, 'unique_terms': 130}, {'doc_id': 595, 'tf': 1, 'max_tf': 9, 'doclen': 124, 'unique_terms': 80}, {'doc_id': 643, 'tf': 1, 'max_tf': 5, 'doclen': 79, 'unique_terms': 57}, {'doc_id': 849, 'tf': 1, 'max_tf': 4, 'doclen': 52, 'unique_terms': 45}, {'doc_id': 873, 'tf': 1, 'max_tf': 6, 'doclen': 101, 'unique_terms': 71}, {'doc_id': 933, 'tf': 1, 'max_tf': 6, 'doclen': 159, 'unique_terms': 123}, {'doc_id': 1196, 'tf': 1, 'max_tf': 5, 'doclen': 60, 'unique_terms': 45}, {'doc_id': 1345, 'tf': 1, 'max_tf': 3, 'doclen': 70, 'unique_terms': 52}]}\n",
      "==================================================\n",
      "comprised\n",
      "{'df': 5, 'documents': ['1111110001110', '1111110101011', '11000', '11111111010001010', '111111100110010'], 'posting_list': [{'doc_id': 78, 'tf': 1, 'max_tf': 5, 'doclen': 120, 'unique_terms': 90}, {'doc_id': 185, 'tf': 1, 'max_tf': 12, 'doclen': 185, 'unique_terms': 112}, {'doc_id': 189, 'tf': 1, 'max_tf': 12, 'doclen': 236, 'unique_terms': 95}, {'doc_id': 583, 'tf': 1, 'max_tf': 4, 'doclen': 80, 'unique_terms': 62}, {'doc_id': 761, 'tf': 1, 'max_tf': 4, 'doclen': 49, 'unique_terms': 35}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que1C\")\n",
    "print('-'*50)\n",
    "for lemma in que1c:\n",
    "    print(lemma)\n",
    "    print(index[lemma])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46d6b9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1C\n",
      "--------------------------------------------------\n",
      "lemma: desired\n",
      "The first gamma compressed gap (the first document ID) in the posting list of the lemma: 111100001\n",
      "The second gamma compressed gap in the posting list of the lemma: 111111101100011\n",
      "The last gamma compressed gap in the posting list of the lemma: 111111100011001\n",
      "==================================================\n",
      "lemma: eliminated\n",
      "The first gamma compressed gap (the first document ID) in the posting list of the lemma: 11111111001010110\n",
      "The second gamma compressed gap in the posting list of the lemma: 111100110\n",
      "The last gamma compressed gap in the posting list of the lemma: 111111100010101\n",
      "==================================================\n",
      "lemma: comprised\n",
      "The first gamma compressed gap (the first document ID) in the posting list of the lemma: 1111110001110\n",
      "The second gamma compressed gap in the posting list of the lemma: 1111110101011\n",
      "The last gamma compressed gap in the posting list of the lemma: 111111100110010\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 1C\")\n",
    "print('-'*50)\n",
    "\n",
    "for lemma in que1c:\n",
    "    print(f\"lemma: {lemma}\")\n",
    "    print(f\"The first gamma compressed gap (the first document ID) in the posting list of the lemma: {index[lemma]['documents'][0]}\")\n",
    "    print(f\"The second gamma compressed gap in the posting list of the lemma: {index[lemma]['documents'][1]}\")\n",
    "    print(f\"The last gamma compressed gap in the posting list of the lemma: {index[lemma]['documents'][-1]}\")\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb75a57",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d83f538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "que2a = [\"cranfield0697\", \"cranfield0971\", \"cranfield0619\"]\n",
    "que2b = [\"immedi\", \"content\", \"cruciform\"]\n",
    "que2c = [\"disagr\", \"loss\", \"accommod\"]\n",
    "\n",
    "# que2a = ['cranfield0449', 'cranfield0093', 'cranfield1178']\n",
    "# que2b = ['reflect', 'acceler', 'unit']\n",
    "# que2c = ['irrevers', 'crosssect', 'block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33d0c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2A\n",
      "--------------------------------------------------\n",
      "cranfield0619\n",
      "doclen: The total number of stems in the document: 25\n",
      "max_tf: The frequency of the most frequent stem in the document: 2\n",
      "unique_terms: The number of unique stems in the document: 22\n",
      "==================================================\n",
      "cranfield0697\n",
      "doclen: The total number of stems in the document: 100\n",
      "max_tf: The frequency of the most frequent stem in the document: 11\n",
      "unique_terms: The number of unique stems in the document: 55\n",
      "==================================================\n",
      "cranfield0971\n",
      "doclen: The total number of stems in the document: 92\n",
      "max_tf: The frequency of the most frequent stem in the document: 8\n",
      "unique_terms: The number of unique stems in the document: 49\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Function to process each document and update the index\n",
    "def get_stems(filename, doc_id, index):\n",
    "    with builtins.open(f'Cranfield/{filename}', 'r') as file:\n",
    "        content = file.read()\n",
    "        stemmer = PorterStemmer()\n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        title = soup.find('title').get_text() if soup.find('title') else ''\n",
    "        text = soup.find('text').get_text() if soup.find('text') else ''\n",
    "        \n",
    "        # Process title and text separately\n",
    "        title_tokens = process_text(title)\n",
    "        text_tokens = process_text(text)\n",
    "\n",
    "        # Combine title and text tokens for each document\n",
    "        document_tokens = title_tokens + text_tokens\n",
    "        \n",
    "        document_stems = [stemmer.stem(token) for token in document_tokens]\n",
    "\n",
    "        max_tf = max(document_stems.count(stem) for stem in set(document_stems))\n",
    "        doclen = len(document_stems)\n",
    "        unique_terms = len(set(document_stems))\n",
    "        \n",
    "        if filename in que2a:\n",
    "            print(filename)\n",
    "            print(f\"doclen: The total number of stems in the document: {doclen}\")\n",
    "            print(f\"max_tf: The frequency of the most frequent stem in the document: {max_tf}\")\n",
    "            print(f\"unique_terms: The number of unique stems in the document: {unique_terms}\")\n",
    "            print(\"=\"*50)\n",
    "        \n",
    "        \n",
    "        # Update posting list for each stem\n",
    "        for stem in set(document_stems):\n",
    "            index_stem[stem]['df'] += 1\n",
    "            if doc_id not in index_stem[stem]['documents']:\n",
    "                index_stem[stem]['documents'].append(doc_id)\n",
    "            index_stem[stem]['posting_list'].append({\n",
    "                'doc_id': doc_id,\n",
    "                'tf': document_stems.count(stem),\n",
    "                'max_tf': max_tf,\n",
    "                'doclen': doclen,\n",
    "                'unique_terms': unique_terms\n",
    "            })\n",
    "        \n",
    "# Initialize the index\n",
    "index_stem = defaultdict(lambda: {'df': 0, 'documents': [], 'posting_list': []})\n",
    "print(\"Question 2A\")\n",
    "print('-'*50)\n",
    "\n",
    "# Path to the Cranfield collection\n",
    "cranfield_path = 'Cranfield/'\n",
    "\n",
    "# Iterate over each document in the Cranfield collection\n",
    "for filename in os.listdir(cranfield_path):\n",
    "    if filename.startswith('cranfield'):\n",
    "        doc_id = int(re.search(r'\\d+', filename).group())  # Extract document ID from the filename\n",
    "        get_stems(filename, doc_id, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7637e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que2B\n",
      "--------------------------------------------------\n",
      "immedi\n",
      "{'df': 18, 'documents': [58, 135, 149, 213, 352, 439, 579, 625, 667, 689, 721, 869, 907, 968, 989, 1088, 1313, 1342], 'posting_list': [{'doc_id': 58, 'tf': 1, 'max_tf': 6, 'doclen': 107, 'unique_terms': 71}, {'doc_id': 135, 'tf': 1, 'max_tf': 6, 'doclen': 99, 'unique_terms': 57}, {'doc_id': 149, 'tf': 1, 'max_tf': 10, 'doclen': 149, 'unique_terms': 97}, {'doc_id': 213, 'tf': 1, 'max_tf': 8, 'doclen': 168, 'unique_terms': 116}, {'doc_id': 352, 'tf': 1, 'max_tf': 5, 'doclen': 120, 'unique_terms': 88}, {'doc_id': 439, 'tf': 1, 'max_tf': 8, 'doclen': 110, 'unique_terms': 66}, {'doc_id': 579, 'tf': 1, 'max_tf': 6, 'doclen': 108, 'unique_terms': 73}, {'doc_id': 625, 'tf': 1, 'max_tf': 7, 'doclen': 194, 'unique_terms': 130}, {'doc_id': 667, 'tf': 2, 'max_tf': 9, 'doclen': 147, 'unique_terms': 95}, {'doc_id': 689, 'tf': 1, 'max_tf': 11, 'doclen': 160, 'unique_terms': 89}, {'doc_id': 721, 'tf': 1, 'max_tf': 15, 'doclen': 300, 'unique_terms': 140}, {'doc_id': 869, 'tf': 1, 'max_tf': 10, 'doclen': 186, 'unique_terms': 118}, {'doc_id': 907, 'tf': 1, 'max_tf': 6, 'doclen': 126, 'unique_terms': 83}, {'doc_id': 968, 'tf': 1, 'max_tf': 8, 'doclen': 101, 'unique_terms': 66}, {'doc_id': 989, 'tf': 1, 'max_tf': 8, 'doclen': 117, 'unique_terms': 61}, {'doc_id': 1088, 'tf': 1, 'max_tf': 4, 'doclen': 121, 'unique_terms': 85}, {'doc_id': 1313, 'tf': 1, 'max_tf': 24, 'doclen': 370, 'unique_terms': 181}, {'doc_id': 1342, 'tf': 1, 'max_tf': 6, 'doclen': 148, 'unique_terms': 101}]}\n",
      "==================================================\n",
      "content\n",
      "{'df': 4, 'documents': [579, 718, 1007, 1096], 'posting_list': [{'doc_id': 579, 'tf': 1, 'max_tf': 6, 'doclen': 108, 'unique_terms': 73}, {'doc_id': 718, 'tf': 1, 'max_tf': 4, 'doclen': 79, 'unique_terms': 68}, {'doc_id': 1007, 'tf': 1, 'max_tf': 5, 'doclen': 104, 'unique_terms': 67}, {'doc_id': 1096, 'tf': 1, 'max_tf': 3, 'doclen': 61, 'unique_terms': 43}]}\n",
      "==================================================\n",
      "cruciform\n",
      "{'df': 8, 'documents': [229, 289, 432, 433, 434, 520, 825, 1202], 'posting_list': [{'doc_id': 229, 'tf': 4, 'max_tf': 10, 'doclen': 125, 'unique_terms': 79}, {'doc_id': 289, 'tf': 7, 'max_tf': 9, 'doclen': 138, 'unique_terms': 76}, {'doc_id': 432, 'tf': 6, 'max_tf': 13, 'doclen': 136, 'unique_terms': 77}, {'doc_id': 433, 'tf': 1, 'max_tf': 19, 'doclen': 249, 'unique_terms': 127}, {'doc_id': 434, 'tf': 1, 'max_tf': 7, 'doclen': 136, 'unique_terms': 80}, {'doc_id': 520, 'tf': 1, 'max_tf': 8, 'doclen': 136, 'unique_terms': 80}, {'doc_id': 825, 'tf': 2, 'max_tf': 5, 'doclen': 127, 'unique_terms': 99}, {'doc_id': 1202, 'tf': 3, 'max_tf': 7, 'doclen': 183, 'unique_terms': 104}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que2B\")\n",
    "print('-'*50)\n",
    "for stem in que2b:\n",
    "    print(stem)\n",
    "    print(index_stem[stem])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ce7f682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que2C\n",
      "--------------------------------------------------\n",
      "disagr\n",
      "{'df': 7, 'documents': [139, 276, 455, 927, 1302, 1352, 1370], 'posting_list': [{'doc_id': 139, 'tf': 1, 'max_tf': 5, 'doclen': 78, 'unique_terms': 54}, {'doc_id': 276, 'tf': 1, 'max_tf': 6, 'doclen': 111, 'unique_terms': 67}, {'doc_id': 455, 'tf': 1, 'max_tf': 6, 'doclen': 118, 'unique_terms': 78}, {'doc_id': 927, 'tf': 1, 'max_tf': 12, 'doclen': 255, 'unique_terms': 140}, {'doc_id': 1302, 'tf': 1, 'max_tf': 6, 'doclen': 104, 'unique_terms': 73}, {'doc_id': 1352, 'tf': 1, 'max_tf': 11, 'doclen': 138, 'unique_terms': 75}, {'doc_id': 1370, 'tf': 1, 'max_tf': 9, 'doclen': 126, 'unique_terms': 83}]}\n",
      "==================================================\n",
      "loss\n",
      "{'df': 23, 'documents': [14, 177, 213, 214, 216, 219, 245, 258, 274, 426, 427, 511, 548, 603, 877, 1089, 1093, 1094, 1195, 1226, 1277, 1325, 1349], 'posting_list': [{'doc_id': 14, 'tf': 1, 'max_tf': 8, 'doclen': 224, 'unique_terms': 174}, {'doc_id': 177, 'tf': 1, 'max_tf': 11, 'doclen': 125, 'unique_terms': 83}, {'doc_id': 213, 'tf': 1, 'max_tf': 8, 'doclen': 168, 'unique_terms': 116}, {'doc_id': 214, 'tf': 2, 'max_tf': 5, 'doclen': 69, 'unique_terms': 45}, {'doc_id': 216, 'tf': 1, 'max_tf': 9, 'doclen': 153, 'unique_terms': 96}, {'doc_id': 219, 'tf': 1, 'max_tf': 6, 'doclen': 128, 'unique_terms': 92}, {'doc_id': 245, 'tf': 1, 'max_tf': 9, 'doclen': 88, 'unique_terms': 55}, {'doc_id': 258, 'tf': 1, 'max_tf': 3, 'doclen': 39, 'unique_terms': 30}, {'doc_id': 274, 'tf': 1, 'max_tf': 6, 'doclen': 143, 'unique_terms': 101}, {'doc_id': 426, 'tf': 1, 'max_tf': 6, 'doclen': 89, 'unique_terms': 60}, {'doc_id': 427, 'tf': 4, 'max_tf': 18, 'doclen': 171, 'unique_terms': 86}, {'doc_id': 511, 'tf': 2, 'max_tf': 8, 'doclen': 88, 'unique_terms': 61}, {'doc_id': 548, 'tf': 1, 'max_tf': 13, 'doclen': 181, 'unique_terms': 93}, {'doc_id': 603, 'tf': 2, 'max_tf': 7, 'doclen': 122, 'unique_terms': 95}, {'doc_id': 877, 'tf': 3, 'max_tf': 4, 'doclen': 38, 'unique_terms': 25}, {'doc_id': 1089, 'tf': 1, 'max_tf': 6, 'doclen': 80, 'unique_terms': 58}, {'doc_id': 1093, 'tf': 1, 'max_tf': 5, 'doclen': 60, 'unique_terms': 41}, {'doc_id': 1094, 'tf': 1, 'max_tf': 6, 'doclen': 98, 'unique_terms': 47}, {'doc_id': 1195, 'tf': 1, 'max_tf': 16, 'doclen': 162, 'unique_terms': 98}, {'doc_id': 1226, 'tf': 1, 'max_tf': 8, 'doclen': 162, 'unique_terms': 91}, {'doc_id': 1277, 'tf': 1, 'max_tf': 8, 'doclen': 148, 'unique_terms': 96}, {'doc_id': 1325, 'tf': 2, 'max_tf': 9, 'doclen': 186, 'unique_terms': 115}, {'doc_id': 1349, 'tf': 1, 'max_tf': 8, 'doclen': 123, 'unique_terms': 83}]}\n",
      "==================================================\n",
      "accommod\n",
      "{'df': 6, 'documents': [168, 518, 974, 1056, 1147, 1239], 'posting_list': [{'doc_id': 168, 'tf': 1, 'max_tf': 6, 'doclen': 140, 'unique_terms': 90}, {'doc_id': 518, 'tf': 3, 'max_tf': 4, 'doclen': 113, 'unique_terms': 80}, {'doc_id': 974, 'tf': 1, 'max_tf': 9, 'doclen': 116, 'unique_terms': 70}, {'doc_id': 1056, 'tf': 1, 'max_tf': 6, 'doclen': 141, 'unique_terms': 94}, {'doc_id': 1147, 'tf': 2, 'max_tf': 12, 'doclen': 244, 'unique_terms': 131}, {'doc_id': 1239, 'tf': 1, 'max_tf': 19, 'doclen': 231, 'unique_terms': 144}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que2C\")\n",
    "print('-'*50)\n",
    "for stem in que2c:\n",
    "    print(stem)\n",
    "    print(index_stem[stem])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fb5212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2B\n",
      "--------------------------------------------------\n",
      "lemma: immedi\n",
      "df: The number of documents that the lemma occurs in: 18\n",
      "The first document ID in the posting list of the lemma: 58\n",
      "The first document's tf in the posting list of the lemma: 1\n",
      "The last document ID in the posting list of the lemma: 1342\n",
      "The last document's tf in the posting list of the lemma: 1\n",
      "==================================================\n",
      "lemma: content\n",
      "df: The number of documents that the lemma occurs in: 4\n",
      "The first document ID in the posting list of the lemma: 579\n",
      "The first document's tf in the posting list of the lemma: 1\n",
      "The last document ID in the posting list of the lemma: 1096\n",
      "The last document's tf in the posting list of the lemma: 1\n",
      "==================================================\n",
      "lemma: cruciform\n",
      "df: The number of documents that the lemma occurs in: 8\n",
      "The first document ID in the posting list of the lemma: 229\n",
      "The first document's tf in the posting list of the lemma: 4\n",
      "The last document ID in the posting list of the lemma: 1202\n",
      "The last document's tf in the posting list of the lemma: 3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 2B\")\n",
    "print('-'*50)\n",
    "\n",
    "for term in que2b:\n",
    "    print(f\"lemma: {term}\")\n",
    "    print(f\"df: The number of documents that the lemma occurs in: {index_stem[term]['df']}\")\n",
    "    print(f\"The first document ID in the posting list of the lemma: {index_stem[term]['documents'][0]}\")\n",
    "    print(f\"The first document's tf in the posting list of the lemma: {index_stem[term]['posting_list'][0]['tf']}\")\n",
    "    print(f\"The last document ID in the posting list of the lemma: {index_stem[term]['documents'][-1]}\")\n",
    "    print(f\"The last document's tf in the posting list of the lemma: {index_stem[term]['posting_list'][-1]['tf']}\")\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "578d0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert posting list to compressed format\n",
    "def compress_posting_list(posting_list):\n",
    "    compressed_list = [posting_list[0]]  # First element remains unchanged\n",
    "    for i in range(1, len(posting_list)):\n",
    "        gap = posting_list[i] - posting_list[i-1]\n",
    "        compressed_list.append(gap)\n",
    "    return compressed_list\n",
    "\n",
    "# Convert all posting lists in the index to compressed format\n",
    "for stem in index_stem:\n",
    "    index_stem[stem]['documents'] = compress_posting_list(index_stem[stem]['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1e2c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que2C\n",
      "--------------------------------------------------\n",
      "disagr\n",
      "{'df': 7, 'documents': [139, 137, 179, 472, 375, 50, 18], 'posting_list': [{'doc_id': 139, 'tf': 1, 'max_tf': 5, 'doclen': 78, 'unique_terms': 54}, {'doc_id': 276, 'tf': 1, 'max_tf': 6, 'doclen': 111, 'unique_terms': 67}, {'doc_id': 455, 'tf': 1, 'max_tf': 6, 'doclen': 118, 'unique_terms': 78}, {'doc_id': 927, 'tf': 1, 'max_tf': 12, 'doclen': 255, 'unique_terms': 140}, {'doc_id': 1302, 'tf': 1, 'max_tf': 6, 'doclen': 104, 'unique_terms': 73}, {'doc_id': 1352, 'tf': 1, 'max_tf': 11, 'doclen': 138, 'unique_terms': 75}, {'doc_id': 1370, 'tf': 1, 'max_tf': 9, 'doclen': 126, 'unique_terms': 83}]}\n",
      "==================================================\n",
      "loss\n",
      "{'df': 23, 'documents': [14, 163, 36, 1, 2, 3, 26, 13, 16, 152, 1, 84, 37, 55, 274, 212, 4, 1, 101, 31, 51, 48, 24], 'posting_list': [{'doc_id': 14, 'tf': 1, 'max_tf': 8, 'doclen': 224, 'unique_terms': 174}, {'doc_id': 177, 'tf': 1, 'max_tf': 11, 'doclen': 125, 'unique_terms': 83}, {'doc_id': 213, 'tf': 1, 'max_tf': 8, 'doclen': 168, 'unique_terms': 116}, {'doc_id': 214, 'tf': 2, 'max_tf': 5, 'doclen': 69, 'unique_terms': 45}, {'doc_id': 216, 'tf': 1, 'max_tf': 9, 'doclen': 153, 'unique_terms': 96}, {'doc_id': 219, 'tf': 1, 'max_tf': 6, 'doclen': 128, 'unique_terms': 92}, {'doc_id': 245, 'tf': 1, 'max_tf': 9, 'doclen': 88, 'unique_terms': 55}, {'doc_id': 258, 'tf': 1, 'max_tf': 3, 'doclen': 39, 'unique_terms': 30}, {'doc_id': 274, 'tf': 1, 'max_tf': 6, 'doclen': 143, 'unique_terms': 101}, {'doc_id': 426, 'tf': 1, 'max_tf': 6, 'doclen': 89, 'unique_terms': 60}, {'doc_id': 427, 'tf': 4, 'max_tf': 18, 'doclen': 171, 'unique_terms': 86}, {'doc_id': 511, 'tf': 2, 'max_tf': 8, 'doclen': 88, 'unique_terms': 61}, {'doc_id': 548, 'tf': 1, 'max_tf': 13, 'doclen': 181, 'unique_terms': 93}, {'doc_id': 603, 'tf': 2, 'max_tf': 7, 'doclen': 122, 'unique_terms': 95}, {'doc_id': 877, 'tf': 3, 'max_tf': 4, 'doclen': 38, 'unique_terms': 25}, {'doc_id': 1089, 'tf': 1, 'max_tf': 6, 'doclen': 80, 'unique_terms': 58}, {'doc_id': 1093, 'tf': 1, 'max_tf': 5, 'doclen': 60, 'unique_terms': 41}, {'doc_id': 1094, 'tf': 1, 'max_tf': 6, 'doclen': 98, 'unique_terms': 47}, {'doc_id': 1195, 'tf': 1, 'max_tf': 16, 'doclen': 162, 'unique_terms': 98}, {'doc_id': 1226, 'tf': 1, 'max_tf': 8, 'doclen': 162, 'unique_terms': 91}, {'doc_id': 1277, 'tf': 1, 'max_tf': 8, 'doclen': 148, 'unique_terms': 96}, {'doc_id': 1325, 'tf': 2, 'max_tf': 9, 'doclen': 186, 'unique_terms': 115}, {'doc_id': 1349, 'tf': 1, 'max_tf': 8, 'doclen': 123, 'unique_terms': 83}]}\n",
      "==================================================\n",
      "accommod\n",
      "{'df': 6, 'documents': [168, 350, 456, 82, 91, 92], 'posting_list': [{'doc_id': 168, 'tf': 1, 'max_tf': 6, 'doclen': 140, 'unique_terms': 90}, {'doc_id': 518, 'tf': 3, 'max_tf': 4, 'doclen': 113, 'unique_terms': 80}, {'doc_id': 974, 'tf': 1, 'max_tf': 9, 'doclen': 116, 'unique_terms': 70}, {'doc_id': 1056, 'tf': 1, 'max_tf': 6, 'doclen': 141, 'unique_terms': 94}, {'doc_id': 1147, 'tf': 2, 'max_tf': 12, 'doclen': 244, 'unique_terms': 131}, {'doc_id': 1239, 'tf': 1, 'max_tf': 19, 'doclen': 231, 'unique_terms': 144}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que2C\")\n",
    "print('-'*50)\n",
    "for stem in que2c:\n",
    "    print(stem)\n",
    "    print(index_stem[stem])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a06134bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Gamma encoding for a given integer\n",
    "def gamma_encode(number):\n",
    "    binary_rep = bin(number)[2:]\n",
    "    offset = binary_rep[1:]\n",
    "    offset_length = len(offset)\n",
    "    unary_code = '1' * offset_length + '0'\n",
    "    gamma_code = unary_code + offset\n",
    "    return gamma_code\n",
    "\n",
    "# Function to perform Gamma encoding on a gap compressed posting list\n",
    "def gamma_encode_posting_list(posting_list):\n",
    "    gamma_encoded_list = []\n",
    "    for gap in posting_list:\n",
    "        # Gamma encode the gap value and append to the encoded list\n",
    "        gamma_encoded_gap = gamma_encode(gap)\n",
    "        gamma_encoded_list.append(gamma_encoded_gap)\n",
    "    return gamma_encoded_list\n",
    "\n",
    "# Apply gamma encoding to the gap compressed posting lists in the inverted index\n",
    "for stem in index_stem:\n",
    "    index_stem[stem]['documents'] = gamma_encode_posting_list(index_stem[stem]['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a21a5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To backtrack Que2C\n",
      "--------------------------------------------------\n",
      "disagr\n",
      "{'df': 7, 'documents': ['111111100001011', '111111100001001', '111111100110011', '11111111011011000', '11111111001110111', '11111010010', '111100010'], 'posting_list': [{'doc_id': 139, 'tf': 1, 'max_tf': 5, 'doclen': 78, 'unique_terms': 54}, {'doc_id': 276, 'tf': 1, 'max_tf': 6, 'doclen': 111, 'unique_terms': 67}, {'doc_id': 455, 'tf': 1, 'max_tf': 6, 'doclen': 118, 'unique_terms': 78}, {'doc_id': 927, 'tf': 1, 'max_tf': 12, 'doclen': 255, 'unique_terms': 140}, {'doc_id': 1302, 'tf': 1, 'max_tf': 6, 'doclen': 104, 'unique_terms': 73}, {'doc_id': 1352, 'tf': 1, 'max_tf': 11, 'doclen': 138, 'unique_terms': 75}, {'doc_id': 1370, 'tf': 1, 'max_tf': 9, 'doclen': 126, 'unique_terms': 83}]}\n",
      "==================================================\n",
      "loss\n",
      "{'df': 23, 'documents': ['1110110', '111111100100011', '11111000100', '0', '100', '101', '111101010', '1110101', '111100000', '111111100011000', '0', '1111110010100', '11111000101', '11111010111', '11111111000010010', '111111101010100', '11000', '0', '1111110100101', '111101111', '11111010011', '11111010000', '111101000'], 'posting_list': [{'doc_id': 14, 'tf': 1, 'max_tf': 8, 'doclen': 224, 'unique_terms': 174}, {'doc_id': 177, 'tf': 1, 'max_tf': 11, 'doclen': 125, 'unique_terms': 83}, {'doc_id': 213, 'tf': 1, 'max_tf': 8, 'doclen': 168, 'unique_terms': 116}, {'doc_id': 214, 'tf': 2, 'max_tf': 5, 'doclen': 69, 'unique_terms': 45}, {'doc_id': 216, 'tf': 1, 'max_tf': 9, 'doclen': 153, 'unique_terms': 96}, {'doc_id': 219, 'tf': 1, 'max_tf': 6, 'doclen': 128, 'unique_terms': 92}, {'doc_id': 245, 'tf': 1, 'max_tf': 9, 'doclen': 88, 'unique_terms': 55}, {'doc_id': 258, 'tf': 1, 'max_tf': 3, 'doclen': 39, 'unique_terms': 30}, {'doc_id': 274, 'tf': 1, 'max_tf': 6, 'doclen': 143, 'unique_terms': 101}, {'doc_id': 426, 'tf': 1, 'max_tf': 6, 'doclen': 89, 'unique_terms': 60}, {'doc_id': 427, 'tf': 4, 'max_tf': 18, 'doclen': 171, 'unique_terms': 86}, {'doc_id': 511, 'tf': 2, 'max_tf': 8, 'doclen': 88, 'unique_terms': 61}, {'doc_id': 548, 'tf': 1, 'max_tf': 13, 'doclen': 181, 'unique_terms': 93}, {'doc_id': 603, 'tf': 2, 'max_tf': 7, 'doclen': 122, 'unique_terms': 95}, {'doc_id': 877, 'tf': 3, 'max_tf': 4, 'doclen': 38, 'unique_terms': 25}, {'doc_id': 1089, 'tf': 1, 'max_tf': 6, 'doclen': 80, 'unique_terms': 58}, {'doc_id': 1093, 'tf': 1, 'max_tf': 5, 'doclen': 60, 'unique_terms': 41}, {'doc_id': 1094, 'tf': 1, 'max_tf': 6, 'doclen': 98, 'unique_terms': 47}, {'doc_id': 1195, 'tf': 1, 'max_tf': 16, 'doclen': 162, 'unique_terms': 98}, {'doc_id': 1226, 'tf': 1, 'max_tf': 8, 'doclen': 162, 'unique_terms': 91}, {'doc_id': 1277, 'tf': 1, 'max_tf': 8, 'doclen': 148, 'unique_terms': 96}, {'doc_id': 1325, 'tf': 2, 'max_tf': 9, 'doclen': 186, 'unique_terms': 115}, {'doc_id': 1349, 'tf': 1, 'max_tf': 8, 'doclen': 123, 'unique_terms': 83}]}\n",
      "==================================================\n",
      "accommod\n",
      "{'df': 6, 'documents': ['111111100101000', '11111111001011110', '11111111011001000', '1111110010010', '1111110011011', '1111110011100'], 'posting_list': [{'doc_id': 168, 'tf': 1, 'max_tf': 6, 'doclen': 140, 'unique_terms': 90}, {'doc_id': 518, 'tf': 3, 'max_tf': 4, 'doclen': 113, 'unique_terms': 80}, {'doc_id': 974, 'tf': 1, 'max_tf': 9, 'doclen': 116, 'unique_terms': 70}, {'doc_id': 1056, 'tf': 1, 'max_tf': 6, 'doclen': 141, 'unique_terms': 94}, {'doc_id': 1147, 'tf': 2, 'max_tf': 12, 'doclen': 244, 'unique_terms': 131}, {'doc_id': 1239, 'tf': 1, 'max_tf': 19, 'doclen': 231, 'unique_terms': 144}]}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"To backtrack Que2C\")\n",
    "print('-'*50)\n",
    "for stem in que2c:\n",
    "    print(stem)\n",
    "    print(index_stem[stem])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e9dc920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2C\n",
      "--------------------------------------------------\n",
      "lemma: disagr\n",
      "The first gamma compressed gap (the first document ID) in the posting list of the lemma: 111111100001011\n",
      "The second gamma compressed gap in the posting list of the lemma: 111111100001001\n",
      "The last gamma compressed gap in the posting list of the lemma: 111100010\n",
      "==================================================\n",
      "lemma: loss\n",
      "The first gamma compressed gap (the first document ID) in the posting list of the lemma: 1110110\n",
      "The second gamma compressed gap in the posting list of the lemma: 111111100100011\n",
      "The last gamma compressed gap in the posting list of the lemma: 111101000\n",
      "==================================================\n",
      "lemma: accommod\n",
      "The first gamma compressed gap (the first document ID) in the posting list of the lemma: 111111100101000\n",
      "The second gamma compressed gap in the posting list of the lemma: 11111111001011110\n",
      "The last gamma compressed gap in the posting list of the lemma: 1111110011100\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 2C\")\n",
    "print('-'*50)\n",
    "\n",
    "for term in que2c:\n",
    "    print(f\"lemma: {term}\")\n",
    "    print(f\"The first gamma compressed gap (the first document ID) in the posting list of the lemma: {index_stem[term]['documents'][0]}\")\n",
    "    print(f\"The second gamma compressed gap in the posting list of the lemma: {index_stem[term]['documents'][1]}\")\n",
    "    print(f\"The last gamma compressed gap in the posting list of the lemma: {index_stem[term]['documents'][-1]}\")\n",
    "    print('='*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
